{
  "topics": [
    {
      "id": "mbafuwe2acp4c9x3rba",
      "title": "Compression",
      "children": [
        {
          "id": "mbah9elkybh5fwotars",
          "title": "AToken is Worth over 1,000 Tokens- Efficient Knowledge Distillation",
          "children": [],
          "expanded": false,
          "parentId": "mbafuwe2acp4c9x3rba"
        },
        {
          "id": "mbaha8wzwk6f5h848v",
          "title": "YT video - Pruning and Distillation Best Practices: The Minitron Approach Explained-",
          "children": [
            {
              "id": "mbaz4hn5p9n7aor7u5",
              "title": "New Subtopic",
              "children": [],
              "expanded": false,
              "parentId": "mbaha8wzwk6f5h848v"
            }
          ],
          "expanded": true,
          "parentId": "mbafuwe2acp4c9x3rba"
        }
      ],
      "expanded": true,
      "parentId": null
    }
  ],
  "currentTopic": {
    "id": "mbaha8wzwk6f5h848v",
    "title": "YT video",
    "children": [
      {
        "id": "mbaz4hn5p9n7aor7u5",
        "title": "New Subtopic",
        "children": [],
        "expanded": false,
        "parentId": "mbaha8wzwk6f5h848v"
      }
    ],
    "expanded": true,
    "parentId": "mbafuwe2acp4c9x3rba"
  },
  "studyMaterials": [
    {
      "id": "mbagecov90qow1trblu",
      "topicId": "mbafuz536rps9gnkz0x",
      "type": "pdf",
      "title": "48b2b323-290b-496c-a1a8-e60c440b27c2.pdf",
      "filename": "448f7a67-78d3-449b-a634-a8d52d8f95c6-48b2b323-290b-496c-a1a8-e60c440b27c2.pdf",
      "isPersistent": true,
      "needsReupload": false,
      "createdAt": "2025-05-30T07:00:21.085Z"
    },
    {
      "id": "mbahav1u25f1cpv3ck2",
      "topicId": "mbah9elkybh5fwotars",
      "type": "pdf",
      "title": "056af1df-8b4d-48af-80d6-4d27b23d7171-AToken is Worth over 1,000 Tokens- Efficient Knowledge Distillation through Low-Rank Clone.pdf",
      "filename": "ef3f2465-9f0f-4fa4-8b61-518f8a0ba438-056af1df-8b4d-48af-80d6-4d27b23d7171-AToken is Worth over 1,000 Tokens- Efficient Knowledge Distillation through Low-Rank Clone.pdf",
      "isPersistent": true,
      "needsReupload": false,
      "createdAt": "2025-05-30T07:25:37.866Z"
    },
    {
      "id": "mbaz6xbqrbnsm3h9x3g",
      "topicId": "mbaha8wzwk6f5h848v",
      "type": "link",
      "title": "Pruning and Distillation Best Practices: The Minitron Approach Explained",
      "createdAt": "2025-05-30T15:46:27.254Z",
      "url": "https://www.youtube.com/watch?v=eJs-8IDHJ3w"
    }
  ],
  "visualizations": [
    {
      "id": "mbagemsq0gn4rdxl4lab",
      "topicId": "mbafuwe2acp4c9x3rba",
      "title": "s",
      "code": "This is part",
      "createdAt": "2025-05-30T07:00:34.154Z"
    },
    {
      "id": "mbazh2tvlgbya72floa",
      "topicId": "mbaha8wzwk6f5h848v",
      "title": "Notes",
      "code": "LLMs usually have sparce parameters, meaning many of them might not hold much value (zeros). Removing this unnecessary parameters (weights) is called Pruning. We also reduce the activation size.\nWe can speed up the model by reducing bit-size process known quantization.\nKnowledge distillation  - \n",
      "createdAt": "2025-05-30T15:54:20.947Z",
      "width": 714.26171875,
      "height": 295.55859375
    }
  ],
  "contentLinks": [],
  "leftPanelVisible": true,
  "rightPanelVisible": true,
  "leftPanelWidth": 50,
  "sidebarCollapsed": false
}